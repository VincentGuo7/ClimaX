seed_everything: 42

# ---------------------------- TRAINER -------------------------------------------
trainer:
  default_root_dir: ${oc.env:OUTPUT_DIR,/workspace/climax_logs}

  limit_val_batches: 1.0

  precision: 16

  num_nodes: 1
  accelerator: gpu
  devices: 1
  strategy: single_device

  min_epochs: 1
  max_epochs: 10
  enable_progress_bar: true

  sync_batchnorm: True
  enable_checkpointing: True
  resume_from_checkpoint: null

  # debugging
  fast_dev_run: false

  logger:
    class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: ${trainer.default_root_dir}/logs
      name: null
      version: null
      log_graph: False
      default_hp_metric: True
      prefix: ""

  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"

    # - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    #   init_args:
    #     dirpath: "${trainer.default_root_dir}/checkpoints"
    #     save_top_k: -1          # Save all checkpoints (optional, set to 1 to save only last)
    #     save_last: true         # Always save the most recent checkpoint
    #     every_n_epochs: 1       # Save every epoch
    #     filename: "epoch_{epoch:03d}"
    #     auto_insert_metric_name: false

    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: "${trainer.default_root_dir}/checkpoints"
        monitor: "val/w_rmse" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        save_top_k: 1 # save k best models (determined by above metric)
        save_last: True # additionaly always save model from last epoch
        verbose: False
        filename: "epoch_{epoch:03d}"
        auto_insert_metric_name: False

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: "val/w_rmse" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        patience: 5 # how many validation epochs of not improving until training stops
        min_delta: 0. # minimum change in the monitored metric needed to qualify as an improvement

    - class_path: pytorch_lightning.callbacks.RichModelSummary
      init_args:
        max_depth: -1

    - class_path: pytorch_lightning.callbacks.RichProgressBar


# ---------------------------- MODEL -------------------------------------------
model:
  lr: 5e-4
  beta_1: 0.9
  beta_2: 0.99
  weight_decay: 1e-5
  warmup_epochs: 10000
  max_epochs: 100000
  warmup_start_lr: 1e-8
  eta_min: 1e-8
  pretrained_path: ./pretrained_checkpoints/1.40625deg.ckpt

  net:
    class_path: climax.regional_forecast.arch.RegionalClimaX
    init_args:
      default_vars: [
        "stl1", "slhf", "u10", "tclw", "skt", "msl", "cvl", 
        "v10", "str", "tcrw", "sp", "ssr", "tcsw", "cbh", "sshf", 
        "tcc", "pev", "stl2", "tcw", "d2m", "tciw", "tcwv", "tp", 
        "cp", "cvh", "tco3", "t2m", "e", "lsp", "fg10"
      ]
      img_size: [143, 69]
      patch_size: 2
      embed_dim: 512
      depth: 8
      decoder_depth: 2
      num_heads: 16
      mlp_ratio: 4
      drop_path: 0.1
      drop_rate: 0.1

# ---------------------------- DATA -------------------------------------------
data:
  root_dir: finetuning_1.40625deg
  variables: ["stl1", "slhf", "u10", "tclw", "skt", "msl", "cvl", 
    "v10", "str", "tcrw", "sp", "ssr", "tcsw", "cbh", "sshf", 
    "tcc", "pev", "stl2", "tcw", "d2m", "tciw", "tcwv", "tp", 
    "cp", "cvh", "tco3", "t2m", "e", "lsp", "fg10"
  ]
  out_variables: ["stl1", "slhf", "u10", "tclw", "skt", "msl", 
    "cvl", "v10", "str", "tcrw", "sp", "ssr", "tcsw", "cbh", 
    "sshf", "tcc", "pev", "stl2", "tcw", "d2m", "tciw", "tcwv", 
    "tp", "cp", "cvh", "tco3", "t2m", "e", "lsp", "fg10"
  ]
  region: "Eastern Australia"
  predict_range: 24
  hrs_each_step: 24
  buffer_size: 10000
  batch_size: 16
  num_workers: 4
  pin_memory: False
